---
title: "人工智能最新资讯"
date: 2023-12-10T08:12:48
tags: ['人工智能', '最新资讯', '技术']
author: Frank Lin
category: Agents
---


## 人工智能最新资讯

### 2023年12月9日

#### @llama_index
一种扩展RAG系统的强有力方式是允许查询结构化和非结构化数据。不仅仅是做top-k，而是编写一个可以针对您的数据系统执行的DSL。 我们现在完全支持以下向量数据库的精确匹配和范围查询：

- [@qdrant_engine](https://github.com/run-llama/llama_index/blob/main/docs/examples/vector_stores/Qdrant_metadata_filter.ipynb)
- [@weaviate_io](https://github.com/run-llama/llama_index/blob/main/docs/examples/vector_stores/WeaviateIndex_metadata_filter.ipynb)
- [@trychroma](https://github.com/run-llama/llama_index/blob/main/docs/examples/vector_stores/chroma_metadata_filter.ipynb)
- [@pinecone](https://github.com/run-llama/llama_index/blob/main/docs/examples/vector_stores/pinecone_metadata_filter.ipynb)

我们还在postgres中有一个特殊案例，其中DSL可以直接提示LLM编写pgvector SQL。

- [pgvector](https://github.com/run-llama/llama_index/blob/main/docs/examples/query_engine/pgvector_sql_query_engine.ipynb)

#### @LangChainAI
🚨新论文警报 - LLMCompiler

如果你喜欢@karpathy在""LLM OS""上的视频，你一定会想看看这篇论文（附带完整的开源存储库！）。这个系统可以在大规模运行中编制执行多个任务的有效计划。

- 论文：[arxiv.org/abs/2312.04511](https://arxiv.org/abs/2312.04511)
- 代码：[github.com/SqueezeAILab/LLMC…](https://github.com/SqueezeAILab/LLMCompiler)

#### @gdb
重要的领导力技能是准确地传达你的意图——也就是提出成功执行的属性，同时留下如何实现这些目标的选择权交由团队提议。

#### @DrJimFan
人类选择了未来更多磁力链接和HF存储库的道路👏。 Mistral磁力链接很棒，但让我们把时间表安排好。我的学生和英伟达实习生Fuzhao在4个月前开源了一个仅解码器的MoE。 Google Switch Transformer，一个基于T5的MoE，已经开源。

以上为中文专栏『人工智能最新资讯』的推文节选，供参考。

---

### 2023年12月10日

@Saboo_Shubham_ 发布了一条有关构建多模态数据聊天机器人的免费网络研讨会信息。LangChain 提供了一个机会，让参与者学习如何使用 LangChain 构建多模态聊天机器人。有兴趣的读者可以在 [这里](https://events.singlestore.com/webinar-beginners-guide-to-langchain-chat-with-your-multi-model-data?utm_source=shubham-saboo&amp;utm_medium=influencer&amp;utm_campaign=Beginners-Guide-Langchain-RSVP) 报名参加。

@Saboo_Shubham_ 还发布了有关 Mistral 8x7B 在 LangSmith Playground 上线的消息。该模型使用了由火箭AI团队的实现，他们通过参数名称逆向工程了该架构。详情请点击 [此链接](https://x.com/i/status/1733672915130847419/video/1) 查看。

### 2023年12月9日

@siddddhesh 分享了一条关于 Data &amp; AI 社区的信息，包括 AI、科学分析以及机器学习和深度学习方面的专家。感兴趣的读者可以在推特上关注这些专家。

@Saboo_Shubham_ 发布了一条关于 Mistral MoE 的修正信息，并提到了 Google 的 switch transformer 和 @XueFz 的 OpenMoE。

@Saboo_Shubham_ 宣传了一个免费网络研讨会，主题是如何创建自己的聊天 LLMs、AI 代理以及使用检索增强生成（RAG）。有兴趣的读者可以在 [这里](https://www.eventbrite.com/e/retrieval-apis-chat-llms-ai-agents-build-llm-apps-at-scale-tickets-773408364237?aff=oddtdtcreator) 登记参加。

@Saboo_Shubham_ 发布了有关 Mistral AI 发布首个开源的 MoE 模型的消息。同时提到了有趣的细节和对 Google 和 Mistral AI 的评价。

### 2023年12月8日

**今天的人工智能新闻就在这里了：**

1. Rowan Cheung分享了Grok的关键见解，并承诺在他的通讯中提供更深入的报告。[点击这里](https://www.therundown.ai/subscribe)加入40万多读者，永远不会错过人工智能领域的任何信息。

2. Grok将拥有可共享的聊天，类似于ChatGPT。[点击这里](https://share.x.ai/conversation/cfd1c5aa-03ad-4bc9-9dac-562be3df51b5)查看Grok的聊天。

3. Grok用户很快将能够提交有关AI回答问题的反馈。这包括撰写最佳回应，以帮助AI随着时间的推移得到更好的训练。

4. Grok拥有独特的个性，可以戏弄𝕏中的任何个人简介。

5. Grok有两种模式，包括常规模式和娱乐模式。娱乐模式下的Grok拥有独特的个性，比ChatGPT的沉闷回应更加“有趣”。

6. Grok不仅可以为您总结新闻，还可以提供引用推文的相关信息。

7. 与ChatGPT（或任何其他LLM）不同，Grok可以访问𝕏上的实时信息。

8. Grok直接集成到𝕏（原Twitter）中，并将在平台内拥有自己独特的标签。

9. 关于如何访问Grok的具体方法。

**希望这些信息对你有所帮助，记得点赞或转发支持内容哦！👍**



        